---
layout: distill
title: Jackknife, Bootstrap and Bayesian Bootstrap
description: Bootstrap
tags: Resampling
giscus_comments: false
date: 2024-07-06
featured: true

bibliography: 2024-07-06-Jackknife-Bootstrap-BayesianBootstrap.bib

toc:
  - name: Jackknife
  - name: Bootstrap
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Bayesian Bootstrap

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
Ways of resampling, comparison of Jackknife, Bootstrap and Bayesian Bootstrap.

## Jackknife <d-cite key="miller1974jackknife"></d-cite>
Given a sample of size $$n$$, a [jackknife estimator](https://en.wikipedia.org/wiki/Jackknife_resampling) can be built by aggregating the parameter estimates from each subsample of size $$(n-1)$$ obtained by omitting one observation.

Useful for bias and variance estimation, a linear approximation of the bootstrap.

## Bootstrap <d-cite key="efron1992bootstrap"></d-cite>

The [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) works by treating inference of the true probability distribution $$J$$, given the original data, as being analogous to an inference of the empirical distribution $$\hat{J}$$, given the resampled data (using random sampling with replacement). The accuracy of inferences regarding $$\hat{J}$$ using the resampled data can be assessed because we know $$\hat{J}$$. If $$\hat{J}$$ is a reasonable approximation to $$J$$, then the quality of inference on $$J$$ can in turn be inferred.

Suppose we have a sample of size $$n$$, $$x_1, \ldots, x_n$$, i.i.d. from a random variable/vector $$X$$. A statistic $$\hat{\phi}$$ is chosen to estimate a parameter $$\phi$$ of the distribution of $$X$$. The bootstrap distribution of $$\hat{\phi}$$ is generated by taking repeated bootstrap replications from $$x_1, \ldots, x_n$$. Each replication from $$x_1, \ldots, x_n$$ is a random sample of size $$n$$ with replacement, and each replication of $$\hat{\phi}$$ is the value of $$\hat{\phi}$$ calculated on the boostrap replicated sample. The bootstrapped distribution of $$\hat{\phi}$$ is generated by considering all possible bootstrap replications of $$\hat{\phi}$$.

A generalization of the jackknife, useful in estimating the properties of an estimand or constructing hypothesis tests.

Pros: straightforward, can be applied to complex sampling designs, control and check the stability.

Cons: rely on assumptions (e.g. independence of samples or large enough of a sample size), time-consuming, lead to inconsistency with finite-sample.

## Bayesian Bootstrap <d-cite key="rubin1981bayesian"></d-cite>

The [Bayesian bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Bayesian_bootstrap) is analogous to the bootstrap.

Each Bayesian bootstrap replication generates a posterior probability for each $$x_i$$ where values of $$X$$ that are not observed have zero posterior probability, just as they have zero probability under the sample cdf. The posterior probability for each of the $$n$$ $$x_i$$ is centered at $$\frac{1}{n}$$ but has variability. Specifically, one Bayesian bootstrap replication is generated by drawing $$(n - 1)$$ uniform $$(0, 1)$$ random variates $$u_1, \ldots, u_{n-1}$$, ordering them, and calculating the gaps $$g_i = u_{(i)}-u_{(i-l)}, i = 1, \lodts, n-1$$ where $$u_{(0)} = 0 and u_{(n)} = 1$$. Then $$g = (g_1, \lodts, g_n)$$ is the vector of probabilities to attach to the data values $$x_1, \ldots, x_n$$ in that Bayesian bootstrap replication. Considering all Bayesian bootstrap replications gives the Bayesian bootstrap distribution of the distribution of $$X$$ and thus of any parameter of this distribution.

---
