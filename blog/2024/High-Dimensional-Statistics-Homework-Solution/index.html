<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> High-Dimensional Statistics Homework Solution | Rufeng Liu </title> <meta name="author" content="Rufeng Liu"> <meta name="description" content="Back up for my TA work"> <meta name="keywords" content="Bayesian nonparametric, Model selection, Statistical computing"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/myfavicon.png?b20f02ad5e865d6bb37e8673b87bf49e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rufeng-liu.github.io/blog/2024/High-Dimensional-Statistics-Homework-Solution/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "High-Dimensional Statistics Homework Solution",
            "description": "Back up for my TA work",
            "published": "October 01, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rufeng</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>High-Dimensional Statistics Homework Solution</h1> <p>Back up for my TA work</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#hw1">HW1</a> </div> <ul> <li> <a href="#problem-1">Problem 1</a> </li> <li> <a href="#problem-2">Problem 2</a> </li> <li> <a href="#problem-3">Problem 3</a> </li> <li> <a href="#problem-4">Problem 4</a> </li> </ul> <div> <a href="#hw2">HW2</a> </div> <ul> <li> <a href="#problem-1">Problem 1</a> </li> </ul> <div> <a href="#midterm">Midterm</a> </div> <ul> <li> <a href="#problem-1">Problem 1</a> </li> <li> <a href="#problem-2">Problem 2</a> </li> </ul> </nav> </d-contents> <h2 id="hw1">HW1</h2> <h3 id="problem-1">Problem 1</h3> <p>For a dataset ${y_i, \boldsymbol{X}_ i}_{i=1}^N$ with $y_i \in \mathbb{R}$, $\boldsymbol{X}_i \in \mathbb{R}^p$, define $\boldsymbol{y}=(y_1,\ldots,y_N)$ and $\boldsymbol{X}=(\boldsymbol{X}_1^T,\ldots,\boldsymbol{X}_N^T)^T\in \mathbb{R}^{N\times p}$. Consider the LASSO estimator</p> \[\hat{\boldsymbol{\beta}} = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \frac{1}{2N} \sum_{i=1}^{N} \omega_i (y_i - \boldsymbol{X}_i^T \boldsymbol{\beta})^2 + \lambda \|\boldsymbol{\beta}\|_1,\] <p>where $\omega_i$ are positive constants and $\sum_{i=1}^N \omega_i = 1$.</p> <h4 id="1-write-down-a-proximal-gradient-descent-algorithm-for-solving-this-problem">1. Write down a <code class="language-plaintext highlighter-rouge">proximal gradient descent algorithm</code> for solving this problem</h4> <p><code class="language-plaintext highlighter-rouge">Algorithm</code></p> <p>The proximal gradient descent algorithm solves the problem by iterating between gradient descent and applying the proximal operator for the $\ell_1$-norm regularization term.</p> <ol> <li> <strong>Initialize</strong> $\beta^{0}$.</li> <li> <p><strong>Repeat</strong> until convergence:</p> <p>For function $f(\boldsymbol{\beta})= \frac{1}{2N} \sum_{i=1}^{N} \omega_i (y_i - \boldsymbol{X}<em>i^T \boldsymbol{\beta})^2 + \lambda |\boldsymbol{\beta}|_1$, we can write $f(\boldsymbol{\beta})= g(\boldsymbol{\beta}) + h(\boldsymbol{\beta})$, where $g(\boldsymbol{\beta})=\frac{1}{2N} \sum</em>{i=1}^{N} \omega_i (y_i - \boldsymbol{X}_i^T \boldsymbol{\beta})^2$ is convex and differentiable, $h(\boldsymbol{\beta}) = \lambda |\boldsymbol{\beta}|_1$ is convex.</p> <p>We make quadratic approximation to $g$, for iteration $t+1$, the <code class="language-plaintext highlighter-rouge">proximal gradient</code> update is</p> <p>\(\boldsymbol{\beta}^{t+1} = \text{prox}_{h,s^t}\left(\boldsymbol{\beta}^{t}-s^t \cdot \nabla g(\boldsymbol{\beta}^{t})\right)\) where</p> \[\nabla g(\boldsymbol{\beta}^{t}) = -\frac{1}{N} \sum_{i=1}^{N} \omega_i X_i (y_i - X_i^T \boldsymbol{\beta}^{t})\] <p>then, the <code class="language-plaintext highlighter-rouge">proximal gradient</code> update takes the form: \(\boldsymbol{\beta}^{t+1} = S_{s^t \lambda}\left(\beta^{t} + s^t \frac{1}{N} \sum_{i=1}^{N} \omega_i X_i (y_i - X_i^T \boldsymbol{\beta}^{t})\right)\) where $s^t &gt; 0$ is the step size and $S_{\lambda}(z)$ is the soft-thresholding operator:</p> \[S_{\lambda}(z) = \text{sgn}(z) \max\{|z| - \lambda, 0\}.\] </li> </ol> <hr> <h2 id="hw2">HW2</h2> <p>Citation <d-cite key="carlin1995bayesian"></d-cite>.</p> <p>Poor choices of the linking density (<code class="language-plaintext highlighter-rouge">pseudopriors</code>) \(p(\boldsymbol{\theta}_j\mid M\neq j)\) will make jumps between models extremely unlikely, so that the convergence of the Gibbs sampling may trapped to one model, which might not be the true one in fact. Good choices will produce \(\boldsymbol{\theta}_j^{(g)}\)-values that are consistent with the data, so that \(p(M=j\mid \boldsymbol{\theta},\boldsymbol{y})\) will still be reasonably large at the next \(M\) update step.</p> <p>If for a particular data set one of the \(p(M=j\mid \boldsymbol{y})\) is extremely large, the \(\pi_j\) may be adjusted to correct the imbalance during the early stage of the algorithm, so that the final value of \(B_{ji}\) reflect the true odds in favour of \(M=j\) suggested by the data.</p> <p>Key point: Use the data to help to select the <code class="language-plaintext highlighter-rouge">pseudopriors</code> but <code class="language-plaintext highlighter-rouge">not</code> the prior, match the <code class="language-plaintext highlighter-rouge">pseudopriors</code> as nearly as possible to the true model-specific posteriors.</p> <h3 id="example">Example</h3> <p>Citation <d-cite key="jauch2021mixture"></d-cite>.</p> <p>Let $I=(a,b)$ with $a,b\in \mathbb{R}\cup \lbrace -\infty,\infty \rbrace$. Suppose $f$ and $g$ are density functions with $\int_{I}f(x)dx=1$, $\int_{I}g(x)dx=1$, and $g&gt;0$ on $I$, with $F$ and $G$ be the corresponding distribution functions. For each $s\in I$, let $g^s$ denote the truncated density function with $g^s(x)=g(x)\mathbf{1}_ {(-\infty,s]}(x)/G(s)$ for $x\in I$. We say that $F$ is smaller than $G$ in the likelihood ratio order, denote $F\leq_{LR} G$, if $f/g$ is monotone non-increasing. It is shown that, $f/g$ is monotone non-increasing and locally absolutely continuous on $I$ if and only if there exists $\theta \in \lbrack 0,1 \rbrack$ and an absolutely continuous distribution function $U$ with density $u$, where $U(a+)=0$ and $U(b-)=1$ such that for $x\in I$,</p> \[f(x)=\theta g(x) + (1-\theta) \int_{a}^{b} g^{s}(x)u(s)ds\] <p>When this mixture representation exists, $\theta=\lim_{x\uparrow b} f(x)/g(x)$, if $\theta\in[0,1)$, $U$ is uniquely determined with $U(x)=\frac{G(x)}{1-\theta}\lbrace \frac{F(x)}{G(x)}-\frac{f(x)}{g(x)} \rbrace$, $x\in I$.</p> <p>If we have $I=\mathbb{R}$, densities $g$ and $u$ can be easily modeled using DP mixtures $\mathrm{DP}(P_0,\alpha)$ together with a Gaussian kernel $\phi_{\mu,\sigma^2}$,</p> \[\begin{aligned} g(\cdot\mid P_1) &amp;= \int_{\mathbb{R}\times \mathbb{R}^{+}} \phi_{\mu,\sigma^2}(\cdot) P_1(d\mu,d\sigma^2), \quad P_1\sim \mathrm{DP}(P_{1,0}, \alpha_1) \\ u(\cdot\mid P_2) &amp;= \int_{\mathbb{R}\times \mathbb{R}^{+}} \phi_{\mu,\sigma^2}(\cdot) P_2(d\mu,d\sigma^2), \quad P_2\sim \mathrm{DP}(P_{2,0}, \alpha_2) \end{aligned}\] <p>Under the truncated stick-breaking representation of DP,</p> \[P_k(\cdot)=\sum_{j=1}^{N} v_{k,j} \left\lbrace \prod_{l=1}^{j-1} (1-v_{k,l}) \right\rbrace \delta_{\mu_{k,j}, \sigma_{k,j}^2} (\cdot), \quad k\in \lbrace 1,2 \rbrace\] <p>where the truncation level $N$ is fixed, $v_{k,j}$ are independent beta random variables for each $j\in \lbrace 1,\ldots, N-1 \rbrace$ with $v_{k,N}=1$, and the atoms $(\mu_{k,j}, \sigma_{k,j}^2)^T$ are independent random vectors distributed according to the base measure $P_{0,k}$. Let $\vec{v}_ k=(v_{k,1}, \ldots, v_{k,N-1})^T$, $\vec{\mu}_ k=(\mu_{k,1}, \ldots, \mu_{k,N})^T$, $\vec{\sigma}_ k ^2=(\sigma_{k,1}^2, \ldots, \sigma_{k,N}^2)^T$, the densities of $G$ and $U$ are</p> \[\begin{aligned} g(x \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) &amp;= \sum_{j=1}^{N} v_{1,j} \left\lbrace \prod_{l=1}^{j-1} (1-v_{1,l}) \right\rbrace \delta_{\mu_{1,j}, \sigma_{1,j}^2} (x) \\ u(x \mid \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2) &amp;= \sum_{j=1}^{N} v_{2,j} \left\lbrace \prod_{l=1}^{j-1} (1-v_{2,l}) \right\rbrace \delta_{\mu_{2,j}, \sigma_{2,j}^2} (x) \end{aligned}\] <p>Choosing a spike-and-slab prior that assigns positive probability to the event $\theta=1$ enable us to do model selection, using the posterior probability of $H_0: F=G$, equivalent to $\theta=1$, versus $H_1: F\leq_{LR} G$, equivalent to $\theta\in[0,1)$. Setting $\theta=(1-\gamma)\tilde{\theta}+\gamma$, $H_0: F=G$ and $H_1: F\leq_{LR} G$ can be identified with the events $\gamma=1$ and $\gamma=0$, respectively. The density of $F$ can be expressed as</p> \[f(x \mid \gamma, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2) = \theta g(x \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) + (1-\theta) \int_{-\infty}^{\infty} g^{s}(x \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2)u(s \mid \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)ds\] <p>the joint distribution of the data and parameters is given by</p> \[\begin{aligned} X_i \mid \gamma, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2 &amp; \stackrel{ind}{\sim} f(\cdot \mid \gamma, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2), \quad i\in \lbrace 1,\ldots, n \rbrace \\ Y_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2 &amp; \stackrel{ind}{\sim} g(\cdot \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2), \quad i\in \lbrace 1,\ldots, m \rbrace \\ v_{1,j} &amp; \stackrel{ind}{\sim} \mathrm{Beta}(1,\alpha), \quad j\in \lbrace 1,\ldots, N-1 \rbrace \\ (\mu_{1,j}, \sigma_{1,j}^2)^T &amp; \stackrel{ind}{\sim} \mathrm{Normal\space Inv-Gamma}(m,c,a_1,a_2), \quad j\in \lbrace 1,\ldots, N \rbrace \\ \tilde{\theta} \mid \gamma = 0 &amp; \sim \mathrm{Beta}(b_1,b_2) \\ v_{2,j} \mid \gamma = 0 &amp; \stackrel{ind}{\sim} \mathrm{Beta}(1,\alpha), \quad j\in \lbrace 1,\ldots, N-1 \rbrace \\ (\mu_{2,j}, \sigma_{2,j}^2)^T \mid \gamma = 0 &amp; \stackrel{ind}{\sim} \mathrm{Normal\space Inv-Gamma}(m,c,a_1,a_2), \quad j\in \lbrace 1,\ldots, N \rbrace \\ \tilde{\theta} \mid \gamma = 1 &amp; \sim \mathrm{Beta}(\breve{b}_{1},\breve{b}_{2}) \\ v_{2,j} \mid \gamma = 1 &amp; \stackrel{ind}{\sim} \mathrm{Beta}(1,\breve{\alpha}), \quad j\in \lbrace 1,\ldots, N-1 \rbrace \\ (\mu_{2,j}, \sigma_{2,j}^2)^T \mid \gamma = 1 &amp; \stackrel{ind}{\sim} \breve{p}_{2,0}(\cdot), \quad j\in \lbrace 1,\ldots, N \rbrace \\ \gamma &amp; \sim \mathrm{Bernoulli}(p_0) \end{aligned}\] <p>The density of $\mathrm{Normal\space Inv-Gamma}(m,c,a_1,a_2)$ is</p> \[\pi_{\mathrm{NI}}(\mu,\sigma^2 \mid m,c,a_1,a_2) = \frac{\sqrt{c}}{\sigma\sqrt{2\pi}}\frac{a_2^{a_1}}{\Gamma(a_1)}(\frac{1}{\sigma^2})^{a_1+1} \mathrm{exp}(-\frac{2a_2+c(\mu-m)^2}{2\sigma^2})\] <p>The density of $\mathrm{Beta}(b_1,b_2)$ is</p> \[\pi_{\mathrm{Beta}}(v \mid b_1, b_2) = \frac{\Gamma(b_1+b_2)}{\Gamma(b_1)\Gamma(b_2)}v^{b_1-1}(1-v)^{b_2-1}\] <p>The priors for $\tilde{\theta}$, $v_{2,j}$, and $(\mu_{2,j}, \sigma_{2,j}^2)^T$ are defined conditionally on $\gamma$ for computational purposes. The priors given $\gamma = 1$ are <code class="language-plaintext highlighter-rouge">pseudopriors</code>. The full conditional distributions for the slice-within-Gibbs sampler are given, with a dash as shorthand notation,</p> <ol> <li> <p>Update $(\vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2)^T$.</p> \[\begin{aligned} \pi(\vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2 \mid \gamma = 0, -) &amp; \propto \left\lbrace \prod_{i=1}^{n} f(X_i \mid \gamma = 0, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2) \right\rbrace \times \left\lbrace \prod_{i=1}^{m} g(Y_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) \right\rbrace \\ &amp; \times \left\lbrace \prod_{j=1}^{N} \pi_{\mathrm{NI}}(\mu_{1,j}, \sigma_{1,j} ^2 \mid m,c,a_1,a_2) \right\rbrace \times \left\lbrace \prod_{j=1}^{N-1} \pi_{\mathrm{Beta}}(v_{1,j} \mid b_1, b_2) \right\rbrace \end{aligned}\] \[\begin{aligned} \pi(\vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2 \mid \gamma = 1, -) &amp; \propto \left\lbrace \prod_{i=1}^{n} g(X_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) \right\rbrace \times \left\lbrace \prod_{i=1}^{m} g(Y_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) \right\rbrace \\ &amp; \times \left\lbrace \prod_{j=1}^{N} \pi_{\mathrm{NI}}(\mu_{1,j}, \sigma_{1,j} ^2 \mid m,c,a_1,a_2) \right\rbrace \times \left\lbrace \prod_{j=1}^{N-1} \pi_{\mathrm{Beta}}(v_{1,j} \mid b_1, b_2) \right\rbrace \end{aligned}\] <p>$f(X_i \mid \gamma = 0, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)$ is evaluated using numerical integration.</p> </li> <li> <p>Update $(\vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)^T$.</p> <p>(1) If $\gamma = 0$, the conditional density</p> \[\begin{aligned} \pi(\vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2 \mid \gamma = 0, -) &amp; \propto \left\lbrace \prod_{i=1}^{n} f(X_i \mid \gamma = 0, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2) \right\rbrace \\ &amp; \times \left\lbrace \prod_{j=1}^{N} \pi_{\mathrm{NI}}(\mu_{2,j}, \sigma_{2,j} ^2 \mid m,c,a_1,a_2) \right\rbrace \times \left\lbrace \prod_{j=1}^{N-1} \pi_{\mathrm{Beta}}(v_{2,j} \mid 1,\breve{\alpha}) \right\rbrace \end{aligned}\] <p>(2) If $\gamma = 1$, sample</p> \[\begin{aligned} v_{2,j} \mid \gamma = 1, - &amp; {\sim} \mathrm{Beta}(1,\breve{\alpha}), \quad j\in \lbrace 1,\ldots, N-1 \rbrace \\ (\mu_{2,j}, \sigma_{2,j}^2)^T \mid \gamma = 1, - &amp; {\sim} \breve{p}_{2,0}(\cdot), \quad j\in \lbrace 1,\ldots, N \rbrace \end{aligned}\] </li> <li> <p>Update $\tilde{\theta}$. For each $i \in \lbrace 1,\ldots,n \rbrace$, a latent variable $R_i$ that associates $X_i$ with one of the two components in the mixture representation is introduced,</p> \[\begin{aligned} X_i \mid R_i = 1, \gamma \in \lbrace 0,1 \rbrace, - &amp; \sim g(\cdot \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) \\ X_i \mid R_i = 0, \gamma = 0, - &amp; \sim \int_{-\infty}^{\infty} g^{s}(\cdot \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2)u(s \mid \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)ds \\ R_i \mid \gamma = 0 &amp; \sim \mathrm{Bernoulli}(\tilde{\theta}) \\ \tilde{\theta} \mid \gamma = 0 &amp; \sim \mathrm{Beta}(b_1,b_2) \\ R_i \mid \gamma = 1 &amp; \sim \delta_1(\cdot) \\ \tilde{\theta} \mid \gamma = 1 &amp; \sim \mathrm{Beta}(\breve{b}_1,\breve{b}_2) \end{aligned}\] <p>As $\mathrm{Pr}(R_i = 0, \gamma = 1) = 0$, $R_i$ is updated with</p> \[\begin{aligned} \mathrm{Pr}(R_i = 1 \mid \gamma = 0, -) &amp;= \frac{\tilde{\theta}g(X_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2)}{\tilde{\theta}g(X_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) + (1- \tilde{\theta})\int_{-\infty}^{\infty} g^{s}(X_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2)u(s \mid \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)ds}\\ \mathrm{Pr}(R_i = 0 \mid \gamma = 0, -) &amp;= 1-\mathrm{Pr}(R_i = 1 \mid \gamma = 0, -)\\ \mathrm{Pr}(R_i = 1 \mid \gamma = 1, -) &amp;= 1 \end{aligned}\] <p>and $\tilde{\theta}$ is updated with</p> \[\begin{aligned} \tilde{\theta} \mid \gamma = 0, - &amp; \sim \mathrm{Beta}(b_1+\sum_{i=1}^{n}R_i, b_2+n-\sum_{i=1}^{n}R_i) \\ \tilde{\theta} \mid \gamma = 1, - &amp; \sim \mathrm{Beta}(\breve{b}_1,\breve{b}_2) \end{aligned}\] </li> <li> <p>Update $\gamma$. The full conditional distribution for $\gamma$ is Bernoulli with</p> \[\begin{aligned} \mathrm{Pr}(\gamma = 0 \mid -) &amp; = \frac{(1-p_0) \prod_{i=1}^{n} f(X_i \mid \gamma = 0, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)}{\mathnormal{Z}_ {\gamma}}\\ \mathrm{Pr}(\gamma = 1 \mid -) &amp; = \frac{p_0 \prod_{i=1}^{n} g(X_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2)}{\mathnormal{Z}_ {\gamma}}\\ \end{aligned}\] <p>where</p> \[\mathnormal{Z}_ {\gamma} = p_0 \prod_{i=1}^{n} g(X_i \mid \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2) + (1-p_0) \prod_{i=1}^{n} f(X_i \mid \gamma = 0, \tilde{\theta}, \vec{v}_ 1, \vec{\mu}_ 1, \vec{\sigma}_ 1 ^2, \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)\] </li> </ol> <p><code class="language-plaintext highlighter-rouge">pseudopriors</code> (conditional on $\gamma = 1$) $\mathrm{Beta}(\breve{b}_ 1, \breve{b}_ 2)$, $\mathrm{Beta}(1,\breve{\alpha})$ and $\breve{p}_ {2,0}(\cdot)$ are defined to resemble the posterior distribution of $\tilde{\theta}$, $\vec{v}_ 2$, and $(\vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)^T$ conditional on $\gamma = 0$. First, fix $\gamma = 0$ and run first three steps in a sampler for $Q$ iterations, the Markov chain output $(\tilde{\theta}_ {(1)}, \vec{v}_ {2,(1)}, \vec{\mu}_ {2,(1)}, \vec{\sigma}_ {2,(1)} ^2)^T, \ldots, (\tilde{\theta}_ {(Q)}, \vec{v}_ {2,(Q)}, \vec{\mu}_ {2,(Q)}, \vec{\sigma}_ {2,(Q)} ^2)^T$ provides an approximation of the posterior distribution of $\tilde{\theta}$, $\vec{v}_ 2$, and $(\vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)^T$ conditional on $\gamma = 0$. Then we use the Markvo chain output to determine the <code class="language-plaintext highlighter-rouge">pseudopriors</code>.</p> <p>For the <code class="language-plaintext highlighter-rouge">pseudopriors</code> of $\tilde{\theta}$,</p> \[(\breve{b}_ 1, \breve{b}_ 2) = \arg\max_ {(a,b)} \prod_{q=1}^{Q} \pi_{\mathrm{Beta}}(\tilde{\theta}_{(q)} \mid a, b)\] <p>For the <code class="language-plaintext highlighter-rouge">pseudopriors</code> of $\vec{v}_ 2$,</p> \[\breve{\alpha} = \frac{1}{Q}\sum_{q=1}^{Q} \arg\max _ {a} \prod_{j=1}^{N-1} \pi_{\mathrm{Beta}}(v_{2,j(q)} \mid 1, a)\] <p>For the <code class="language-plaintext highlighter-rouge">pseudopriors</code> of $(\vec{\mu}_ 2, \vec{\sigma}_ 2 ^2)^T$, $\breve{p}_ {2,0}(\cdot)$ is set as a kernel density estimate using the function $\it{kde}$ in the $\boldsymbol{R}$ package $\it{ks}$ computed from $(m_1,s_1^2)^T, \ldots, (m_Q,s_Q^2)^T$, where</p> \[(m_q,s_q^2) \sim u_{(q)}(\cdot \mid \vec{v}_ 2, \vec{\mu}_ 2, \vec{\sigma}_ 2 ^2) = \sum_{j=1}^{N} v_{2,j(q)} \left\lbrace \prod_{l=1}^{j-1} (1-v_{2,l(q)}) \right\rbrace \delta_{\mu_{2,j(q)}, \sigma_{2,j(q)}^2} (\cdot)\] <h3 id="in-project">In project</h3> <p>To test the correlation between different components in different simplexes is equivalent to do model selection between these two models:</p> <ol> <li>Dependent simplexes:</li> </ol> \[g_1(\cdot\mid k_1,k_2,F)\] <ol> <li>Independent simplexes:</li> </ol> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Rufeng Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 02, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>