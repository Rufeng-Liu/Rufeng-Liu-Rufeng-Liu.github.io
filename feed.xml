<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rufeng-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rufeng-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-27T03:53:45+00:00</updated><id>https://rufeng-liu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Bayesian Model Selection via MCMC</title><link href="https://rufeng-liu.github.io/blog/2024/Bayesian-Model-Selection-via-MCMC/" rel="alternate" type="text/html" title="Bayesian Model Selection via MCMC"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://rufeng-liu.github.io/blog/2024/Bayesian-Model-Selection-via-MCMC</id><content type="html" xml:base="https://rufeng-liu.github.io/blog/2024/Bayesian-Model-Selection-via-MCMC/"><![CDATA[<h2 id="method">Method</h2> <p>Choose between \(K\) models with corresponding parameter vector \(\boldsymbol{\theta}_j\), \(j=1,...K\). Let \(M\) be an integer-valued parameter that indexes the model, for model \(j\), we have a likelihood \(f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,M=j)\) and a prior \(p(\boldsymbol{\theta}_j\mid M=j)\). Given \(M=j\), \(\boldsymbol.{y}\) is independent of \(\{\boldsymbol{i\neq j}\}\). Assume that given the indicator \(M\), \(\boldsymbol{\theta}_j\) are independent of each other, we can complete the Bayesian model specification by choosing proper <code class="language-plaintext highlighter-rouge">pseudopriors</code> \(p(\boldsymbol{\theta}_j\mid M\neq j)\), which is a conveniently chosen linking density. Reason shows below, \(p(\boldsymbol{y} \mid M=j)=\int f(\boldsymbol{y}\mid \boldsymbol{\theta},M=j)p(\boldsymbol{\theta}\mid M=j)d\boldsymbol{\theta}=\int f(\boldsymbol{y}\mid \boldsymbol{\theta}_{j},M=j)p(\boldsymbol{\theta}_{j}\mid M=j)d\boldsymbol{\theta}\) Given prior model probabilities \(\pi_{j}\equiv P(M=j)\) such that \(\sum_{j=1}^{K}\pi_{j}=1\), let \(\boldsymbol{\theta}=\{\boldsymbol{\theta}_1,\ldots,\boldsymbol{\theta}_K\}\), when \(M=j\), the joint distribution of \(\boldsymbol{y}\) and \(\boldsymbol{\theta}\) is</p> <p>\(\begin{aligned} p(\boldsymbol{y},\boldsymbol{\theta},M=j) &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta},m=j)p(\boldsymbol{\theta},M=j)p(M=j) \\ &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,m=j)p(\boldsymbol{\theta},M=j)p(M=j) \\ &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,m=j) \prod_{i=1}^{K} p(\boldsymbol{\theta}_j,M=j) p(M=j)\\ &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,m=j) \prod_{i=1}^{K} p(\boldsymbol{\theta}_j,M=j) \pi_{j} \end{aligned}\) To implement Gibbs sampler the full conditional distributions of each \(\boldsymbol{\theta}_j\) and \(M\). For \(\boldsymbol{\theta}_j\): \(p(\boldsymbol{\theta}_j \mid \boldsymbol{\theta}_({i\neq j},M,\boldsymbol{y}) = \begin{cases} f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,M=j)p(\boldsymbol{\theta}_j\mid M = j) &amp; M=j, \\ p(\boldsymbol{\theta}_j\mid M\neq j) &amp; M\neq j, \end{cases}\) For \(M\):</p> <hr/> <h2 id="implementaton">Implementaton</h2> <hr/>]]></content><author><name></name></author><category term="Bayesian"/><category term="model/variable"/><category term="selection"/><summary type="html"><![CDATA[Bayesian model selection]]></summary></entry></feed>