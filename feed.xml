<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rufeng-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rufeng-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-13T22:03:01+00:00</updated><id>https://rufeng-liu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Horseshoe estimator</title><link href="https://rufeng-liu.github.io/blog/2024/Horseshoe-estimator/" rel="alternate" type="text/html" title="Horseshoe estimator"/><published>2024-07-09T00:00:00+00:00</published><updated>2024-07-09T00:00:00+00:00</updated><id>https://rufeng-liu.github.io/blog/2024/Horseshoe-estimator</id><content type="html" xml:base="https://rufeng-liu.github.io/blog/2024/Horseshoe-estimator/"><![CDATA[<p>Introduction to Horseshoe estimator/prior, comparison versus other priors like spike-and-slab.</p> <h2 id="horseshoe">Horseshoe</h2> <p>Citation <d-cite key="carvalho2010horseshoe"></d-cite></p> <h2 id="properties">Properties</h2> <h2 id="comparison">Comparison</h2> <hr/>]]></content><author><name></name></author><category term="Bayesian-model/variable-selection"/><category term="Horseshoe"/><summary type="html"><![CDATA[Horseshoe for sparsity]]></summary></entry><entry><title type="html">Jackknife, Bootstrap and Bayesian Bootstrap</title><link href="https://rufeng-liu.github.io/blog/2024/Jackknife-Bootstrap-BayesianBootstrap/" rel="alternate" type="text/html" title="Jackknife, Bootstrap and Bayesian Bootstrap"/><published>2024-07-06T00:00:00+00:00</published><updated>2024-07-06T00:00:00+00:00</updated><id>https://rufeng-liu.github.io/blog/2024/Jackknife-Bootstrap-BayesianBootstrap</id><content type="html" xml:base="https://rufeng-liu.github.io/blog/2024/Jackknife-Bootstrap-BayesianBootstrap/"><![CDATA[<p>Ways of resampling, comparison of Jackknife, Bootstrap and Bayesian Bootstrap.</p> <h2 id="jackknife-">Jackknife <d-cite key="miller1974jackknife"></d-cite></h2> <p>Given a sample of size \(n\), a <a href="https://en.wikipedia.org/wiki/Jackknife_resampling">jackknife estimator</a> can be built by aggregating the parameter estimates from each subsample of size \((n-1)\) obtained by omitting one observation.</p> <p>Useful for bias and variance estimation, a linear approximation of the bootstrap.</p> <h2 id="bootstrap-">Bootstrap <d-cite key="efron1992bootstrap"></d-cite></h2> <p>The <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap</a> works by treating inference of the true probability distribution \(J\), given the original data, as being analogous to an inference of the empirical distribution \(\hat{J}\), given the resampled data (using random sampling with replacement). The accuracy of inferences regarding \(\hat{J}\) using the resampled data can be assessed because we know \(\hat{J}\). If \(\hat{J}\) is a reasonable approximation to \(J\), then the quality of inference on \(J\) can in turn be inferred.</p> <p>Suppose we have a sample of size \(n\), \(x_1, \ldots, x_n\), i.i.d. from a random variable/vector \(X\). A statistic \(\hat{\phi}\) is chosen to estimate a parameter \(\phi\) of the distribution of \(X\). The bootstrap distribution of \(\hat{\phi}\) is generated by taking repeated bootstrap replications from \(x_1, \ldots, x_n\). Each replication from \(x_1, \ldots, x_n\) is a random sample of size \(n\) with replacement, and each replication of \(\hat{\phi}\) is the value of \(\hat{\phi}\) calculated on the boostrap replicated sample. The bootstrapped distribution of \(\hat{\phi}\) is generated by considering all possible bootstrap replications of \(\hat{\phi}\).</p> <p>A generalization of the jackknife, useful in estimating the properties of an estimand or constructing hypothesis tests.</p> <p>Pros: straightforward, can be applied to complex sampling designs, control and check the stability.</p> <p>Cons: rely on assumptions (e.g. independence of samples or large enough of a sample size), time-consuming, lead to inconsistency with finite-sample.</p> <h2 id="bayesian-bootstrap-">Bayesian Bootstrap <d-cite key="rubin1981bayesian"></d-cite></h2> <p>The <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Bayesian_bootstrap">Bayesian bootstrap</a> is analogous to the bootstrap.</p> <p>Each Bayesian bootstrap replication generates a posterior probability for each \(x_i\), where values of \(X\) that are not observed have zero posterior probability, just as they have zero probability under the sample cdf. The posterior probability for each \(x_i\) is centered at \(\frac{1}{n}\) but has variability. The way of generating is like drawing from a \(n-1\) variate Dirichlet distribution with parameter vector \((1, \ldots, 1)\). Specifically, one Bayesian bootstrap replication is generated by drawing \((n - 1)\) uniform \((0, 1)\) random variates \(u_1, \ldots, u_{n-1}\), ordering them, and calculating the gaps \(g_i = u_{(i)}-u_{(i-l)}, i = 1, \ldots, n-1\) where \(u_{(0)} = 0\) and \(u_{(n)} = 1\). Then \(g = (g_1, \ldots, g_n)\) is the vector of probabilities to attach to the data values \(x_1, \ldots, x_n\) in that Bayesian bootstrap replication. Considering all Bayesian bootstrap replications gives the Bayesian bootstrap distribution of the distribution of \(X\) and thus of any parameter of this distribution.</p> <p>The interpretations of the resulting distributions will be different because the Bayesian bootstrap simulates the posterior distribution of the parameter \(\phi\), whereas the bootstrap simulates the estimated sampling distribution of a statistic \(\hat{\phi}\) estimating \(\phi\). The Bayesian bootstrap has an inherent advantage over the bootstrap with respect to the resulting inferences about parameters: the Bayesian bootstrap generates likelihood statements about parameters rather than frequency statements about statistics under assumed values for parameters.</p> <hr/>]]></content><author><name></name></author><category term="Resampling"/><summary type="html"><![CDATA[Bootstrap]]></summary></entry><entry><title type="html">Variational Inference</title><link href="https://rufeng-liu.github.io/blog/2024/Variational-Inference/" rel="alternate" type="text/html" title="Variational Inference"/><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://rufeng-liu.github.io/blog/2024/Variational-Inference</id><content type="html" xml:base="https://rufeng-liu.github.io/blog/2024/Variational-Inference/"><![CDATA[<p>Use optimization rather than use sampling. First posit a family of densities, then to find a member of that family which is close to the target density. Use exponential family as an example.</p> <h2 id="exponential-families">Exponential families</h2> <p><a href="https://en.wikipedia.org/wiki/Exponential_family">Exponential families</a> include <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal</a>, <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal</a>, <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential</a>, <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">inverse Gaussian</a>, <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared</a>, <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta</a>, <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a>, <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a>, <a href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical</a>, <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a>, <a href="https://en.wikipedia.org/wiki/Wishart_distribution">Wishart</a>, <a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution">inverse Wishart</a>, <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric</a>, <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial</a>(with fixed number of failures), <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial</a>(with fixed number of failures), <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial</a>(with fixed number of failures), <a href="https://en.wikipedia.org/wiki/Weibull_distribution">Weibull</a>(with fixed shape parameter)â€¦</p> <p>For variable \(\boldsymbol{x}=(x_1,\ldots,x_k)^{T}\), a family of distributions with paramter \(\boldsymbol{\theta}\equiv (\theta_1,\ldots,\theta_s)^{T}\) is said to belong to an exponential family if the p.d.f (or p.m.f) can be written as</p> \[f_X(\boldsymbol{X}\mid\boldsymbol{\theta})=h(\boldsymbol{x})exp\left(\sum_{i=1}^{s} \eta_{i}(\boldsymbol{\theta})T_i(\boldsymbol{x})-A(\boldsymbol{\theta})\right)\] <p>or campactly</p> \[f_X(\boldsymbol{X}\mid\boldsymbol{\theta})=h(\boldsymbol{x})exp\left(\boldsymbol{\eta}(\boldsymbol{\theta})\cdot T(\boldsymbol{x})-A(\boldsymbol{\theta})\right)\] <p>The dimensions \(k\) of the random variable need not match the dimension \(d\) of the parameter vector, nor (in the case of a curved exponential function) the dimension \(s\) of the natural parameter \(\boldsymbol{\eta}\) and sufficient statistic \(T(\boldsymbol{x})\) .</p> <h2 id="dirichlet-process-and-dirichlet-process-mixture-">Dirichlet process and Dirichlet process mixture <d-cite key="ferguson1973bayesian"></d-cite></h2> <p>A Dirichlet process \(G\) is parameterized by a centering measure \(G_0\) and a positive presicion/scaling parameter \(\alpha\). The stick-breaking representation <d-cite key="sethuraman1994constructive"></d-cite> is widely used. Suppose we independently draw \(N\) random variables \(\eta_n\) from \(G\):</p> \[\begin{aligned} G\mid G_0,\alpha &amp;\sim \text{DP}(G_0,\alpha)\\ \eta_n &amp;\sim G, \quad n\in\{1,\ldots,N\}. \end{aligned}\] <p>Given Dirichlet process \(G\), a DP mixtures are densities \(p(x)=\int p(x, \eta)d\eta\), or we can have non-i.i.d observations \(x_n\overset{ind}{\sim}p_{n,G}(x)=\int p(x;\eta)dG(\eta)\), in terms of \(N\) latent variables \(\eta_1,\ldots,\eta_N\), the model can be written as</p> \[x_n\mid\eta_n,G\overset{ind}{\sim}p_n(\cdot;\eta_n), \quad \eta_n\mid G\overset{i.i.d}{\sim}G, \quad G\mid G_0,\alpha \sim \text{DP}(G_0,\alpha)\] <p>Given a sample \(\{x_1,\ldots,x_N\}\) from a DP mixture, the predictive density is</p> \[p(x\mid x_1,\ldots,x_N,\alpha,G_0)=\int p(x\mid \eta)p(\eta\mid x_1,\ldots,x_N,\alpha,G_0)d\eta\] <p>which we can use MCMC to achieve posterior draws, together with posterior distribution \(p(\eta\mid x_1,\ldots,x_N,\alpha,G_0)\).</p> <h2 id="inference-">Inference <d-cite key="blei2017variational"></d-cite></h2> <h2 id="implementation--">Implementation <d-cite key="blei2017variational"></d-cite> <d-cite key="blei2006variational"></d-cite></h2> <hr/>]]></content><author><name></name></author><category term="Optimization"/><category term="Variational"/><category term="Dirichlet-process"/><summary type="html"><![CDATA[Variational Bayesian]]></summary></entry><entry><title type="html">Bayesian Model Selection via MCMC</title><link href="https://rufeng-liu.github.io/blog/2024/Bayesian-Model-Selection-via-MCMC/" rel="alternate" type="text/html" title="Bayesian Model Selection via MCMC"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://rufeng-liu.github.io/blog/2024/Bayesian-Model-Selection-via-MCMC</id><content type="html" xml:base="https://rufeng-liu.github.io/blog/2024/Bayesian-Model-Selection-via-MCMC/"><![CDATA[<h2 id="method">Method</h2> <p>Citation <d-cite key="carlin1995bayesian"></d-cite>.</p> <p>Choose between \(K\) models with corresponding parameter vector \(\boldsymbol{\theta}_j\), \(j=1,...K\).</p> <p>Let \(M\) be an integer-valued parameter that indexes the model, for model \(j\), we have a likelihood \(f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,M=j)\) and a prior \(p(\boldsymbol{\theta}_j\mid M=j)\). Given \(M=j\), \(\boldsymbol{y}\) is independent of \(\{\boldsymbol{\theta_{i\neq j}}\}\). Assume that given the indicator \(M\), \(\boldsymbol{\theta}_j\) are independent of each other, we can complete the Bayesian model specification by choosing proper <code class="language-plaintext highlighter-rouge">pseudopriors</code> \(p(\boldsymbol{\theta}_j\mid M\neq j)\), which is a conveniently chosen linking density. Reason is shown below, let \(\boldsymbol{\theta}=\{\boldsymbol{\theta}_1,\ldots,\boldsymbol{\theta}_K\}\), \(p(\boldsymbol{y} \mid M=j)=\int f(\boldsymbol{y}\mid \boldsymbol{\theta},M=j)p(\boldsymbol{\theta}\mid M=j)d\boldsymbol{\theta}=\int f(\boldsymbol{y}\mid \boldsymbol{\theta}_{j},M=j)p(\boldsymbol{\theta}_{j}\mid M=j)d\boldsymbol{\theta}_j\) Given prior model probabilities \(\pi_{j}\equiv P(M=j)\) such that \(\sum_{j=1}^{K}\pi_{j}=1\), when \(M=j\), the joint distribution of \(\boldsymbol{y}\) and \(\boldsymbol{\theta}\) is</p> \[\begin{aligned} p(\boldsymbol{y},\boldsymbol{\theta},M=j) &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta},m=j)p(\boldsymbol{\theta}\mid M=j)p(M=j) \\ &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,m=j)p(\boldsymbol{\theta}\mid M=j)p(M=j) \\ &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,m=j) \left\{\prod_{i=1}^{K} p(\boldsymbol{\theta}_i\mid M=j)\right\} p(M=j)\\ &amp; = f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,m=j) \left\{\prod_{i=1}^{K} p(\boldsymbol{\theta}_i\mid M=j)\right\} \pi_{j} \end{aligned}\] <p>To implement Gibbs sampler the full conditional distributions of each \(\boldsymbol{\theta}_j\) and \(M\). For \(\boldsymbol{\theta}_j\), when \(M=j\), we generate from the usual model \(j\) full conditional; when \(M\neq j\), we generate from the linking function (<code class="language-plaintext highlighter-rouge">pseudoprior</code>).</p> \[p(\boldsymbol{\theta}_j \mid \boldsymbol{\theta}_{i\neq j},M,\boldsymbol{y}) \propto \begin{cases} f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,M=j)p(\boldsymbol{\theta}_j\mid M = j) &amp; M=j, \\ p(\boldsymbol{\theta}_j\mid M\neq j) &amp; M\neq j, \end{cases}\] <p>For discrete finite parameter \(M\):</p> \[p(M=j\mid \boldsymbol{\theta},\boldsymbol{y})=\frac{f(\boldsymbol{y}\mid \boldsymbol{\theta}_j,M=j)\prod_{i=1}^{K} p(\boldsymbol{\theta}_i\mid M=j) \pi_j}{\sum_{k=1}^{K} \left(f(\boldsymbol{y}\mid \boldsymbol{\theta}_k,M=k)\prod_{i=1}^{K} p(\boldsymbol{\theta}_i\mid M=k) \pi_k \right)}\] <p>The algorithm will produce samples from the correct joint posterior distribution. The ratio</p> \[\hat{p}(M=j\mid \boldsymbol{y})=\frac{\text{number of }M^{(g)}=j}{\text{total number of }M^{(g)}},\quad j=1,\ldots,K.\] <p>provides estimates that be used to compute the Bayes factor (ratio of the observed marginal densities for the two models)</p> \[B_{ji}=\frac{p(\boldsymbol{y}\mid M=j)}{p(\boldsymbol{y}\mid M=i)}\] <p>between any two of the models.</p> <hr/> <h2 id="implementation">Implementation</h2> <p>Citation <d-cite key="carlin1995bayesian"></d-cite> <d-cite key="jauch2021mixture"></d-cite>.</p> <p>Poor choices of the linking density (<code class="language-plaintext highlighter-rouge">pseudopriors</code>) \(p(\boldsymbol{\theta}_j\mid M\neq j)\) will make jumps between models extremely unlikely, so that the convergence of the Gibbs sampling may trapped to one model, which might not be the true one in fact. Good choices will produce \(\boldsymbol{\theta}_j^{(g)}\)-values that are consistent with the data, so that \(p(M=j\mid \boldsymbol{\theta},\boldsymbol{y})\) will still be reasonably large at the next \(M\) update step.</p> <p>If for a particular data set one of the \(p(M=j\mid \boldsymbol{y})\) is extremely large, the \(\pi_j\) may be adjusted to correct the imbalance during the early stage of the algorithm, so that the final value of \(B_{ji}\) reflect the true odds in favour of \(M=j\) suggested by the data.</p> <p>Key point: Use the data to help to select the <code class="language-plaintext highlighter-rouge">pseudopriors</code> but <code class="language-plaintext highlighter-rouge">not</code> the prior, match the <code class="language-plaintext highlighter-rouge">pseudopriors</code> as nearly as possible to the true model-specific posteriors.</p> <hr/>]]></content><author><name></name></author><category term="Bayesian-model/variable-selection"/><summary type="html"><![CDATA[Bayesian model selection]]></summary></entry></feed>